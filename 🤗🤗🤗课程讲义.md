# ğŸ¤—ğŸ¤—ğŸ¤—è¯¾ç¨‹è®²ä¹‰

å®‹æ›¦è½© è®¡ç®—æœºç³»

# è¯¾å‰å‡†å¤‡

ç†è®ºéƒ¨åˆ†å»ºè®®å…ˆä¿® [Mathematical Foundations of Machine Learning](https://oi-wiki.org/math/linear-algebra/)

å®è·µéƒ¨åˆ†å»ºè®®äº†è§£ Python, Pytorch

## è½¯ä»¶å‡†å¤‡ï¼š

å»ºè®®ä½¿ç”¨ Linux ğŸ§ 

å®‰è£…ä»¥ä¸‹ package:

[PyTorch](https://pytorch.org/)

[ğŸ¤—transformers](https://huggingface.co/docs/transformers/installation)

```
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simplepip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
```

```requirements.txt
protobuf
transformers
cpm_kernels
torch
gradio
mdtex2html
sentencepiece
accelerate
sse-starlette
streamlit
datasets
peft
tqdm
bitsandbytes
scipy
```

ä¸‹è½½ä»¥ä¸‹æ¨¡å‹æƒé‡ï¼ˆå¯é€‰ï¼‰ï¼š

[gpt2 at main (huggingface.co)](https://huggingface.co/gpt2/tree/main)

[bert-base-chinese at main (huggingface.co)](https://huggingface.co/bert-base-chinese/tree/main)

[THUDM/chatglm2-6b-int4 at main (huggingface.co)](https://huggingface.co/THUDM/chatglm2-6b-int4/tree/main)

å¦‚æœå¸Œæœ›ä½¿ç”¨CPUæ¨ç†chatglmï¼Œå¯ä»¥ä¸‹è½½ï¼š

[THUDM/chatglm-6b-int4 at main (huggingface.co)](https://huggingface.co/THUDM/chatglm-6b-int4/tree/main)

## ç¡¬ä»¶å‡†å¤‡ï¼š

æœ¬æ¬¡è¯¾ç¨‹çš„å®è·µä¸­çš„éƒ¨åˆ†å†…å®¹éœ€è¦GPUèµ„æºï¼Œå¦‚æœç¼ºå°‘æœ¬åœ°ç®—åŠ›ï¼Œå¯ä»¥å°è¯•[Google Colab](https://colab.research.google.com/)ï¼Œæˆ–è€…å°è¯•ä½¿ç”¨CPUè¿›è¡Œè®­ç»ƒã€‚

## æ•°æ®å‡†å¤‡ï¼š

è¯·æå‰ä¸‹è½½ï¼š[openchat/openchat_sharegpt4_dataset at main (huggingface.co)](https://huggingface.co/datasets/openchat/openchat_sharegpt4_dataset/tree/main) ä¸­çš„`sharegpt_gpt4.json`

## æ€æƒ³å‡†å¤‡ï¼ˆå¯é€‰ï¼‰ï¼š

é˜…è¯» [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)

# ç†è®ºéƒ¨åˆ†ï¼šè‡ªç„¶è¯­è¨€å¤„ç†

æœ¬éƒ¨åˆ†å†…å®¹å¯¹ğŸ¤—å¹¶æ— å®é™…å¸®åŠ©ï¼Œå¦‚æœæ‚¨å¯¹æœ¬éƒ¨åˆ†ä¸æ„Ÿå…´è¶£ï¼Œå¯ä»¥ç›´æ¥è·³è½¬ğŸ¤—ã€‚å¦‚æœæ‚¨åœ¨å­¦ä¹ æœ¬å†…å®¹æ—¶å‡ºç°å¤±è¯­ã€è¯­è¨€éšœç¢ã€[MASK]ç­‰ç—‡çŠ¶ï¼Œè¯·å‡å°å­¦ä¹ ç‡ï¼Œå¹¶ä¸èº«è¾¹åŒå­¦åŠæ—¶æ²Ÿé€šï¼Œä»¥å¸®åŠ©æ¢å¤è¯­è¨€èƒ½åŠ›ã€‚

ä»¥ä¸‹`1-4`å°èŠ‚ä»‹ç»äº† transformer çš„åŸç†ï¼Œè·³è¿‡æœ¬éƒ¨åˆ†å¹¶ä¸å½±å“ğŸ¤—ã€‚

## 1. å¦‚ä½•è¡¨ç¤ºä¸€ä¸ªè¯çš„å«ä¹‰ï¼Ÿ (Word Vector / Word Embedding)

## 2. å¦‚ä½•è·å–è¯åœ¨å¥å­ä¸­çš„å‘é‡è¡¨ç¤ºï¼Ÿ (Attention)

[An Apple a Day Keeps the Doctor Away](https://read.qxmd.com/read/25822137/association-between-apple-consumption-and-physician-visits-appealing-the-conventional-wisdom-that-an-apple-a-day-keeps-the-doctor-away)

ä¸€å¤©ä¸€éƒ¨ Iphone è®©æˆ‘ä¸åšå£«å­¦ä½å¤±ä¹‹äº¤è‡‚ï¼Ÿ

[[1706.03762] Attention Is All You Need (arxiv.org)](https://arxiv.org/abs/1706.03762)

## 3. å¦‚ä½•åœ¨æ¨¡å‹ä¸­å­˜å‚¨çŸ¥è¯†ğŸ§€ï¼Ÿ(MLP)

[[2012.14913] Transformer Feed-Forward Layers Are Key-Value Memories (arxiv.org)](https://arxiv.org/abs/2012.14913)

## 4. å¦‚ä½•åŸºäºä»¥ä¸ŠåŸç†æ„å»ºè¯­è¨€æ¨¡å‹ï¼Ÿ(transformer)

[[1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (arxiv.org)](https://arxiv.org/abs/1810.04805)

[Language Models are Unsupervised Multitask Learners (openai.com)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

[[2103.10360] GLM: General Language Model Pretraining with Autoregressive Blank Infilling (arxiv.org)](https://arxiv.org/abs/2103.10360)

ä»¥ä¸‹`5-8`å°èŠ‚ä»‹ç»äº†è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ–¹æ³•ï¼Œå¯ä»¥è·³è¿‡ç†è®ºè®²è§£ï¼Œåœ¨ğŸ¤—ğŸ¤—ğŸ¤—çš„è¿‡ç¨‹ä¸­ğŸ¤—ã€‚

## 5. å¦‚ä½•è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Ÿ(Pretrain)

## 6. å¦‚ä½•ä½¿ç”¨è¯­è¨€æ¨¡å‹å®Œæˆå…·ä½“ä¸‹æ¸¸ä»»åŠ¡ï¼Ÿ(Finetune)

## 7. å¦‚ä½•ä½¿ç”¨æœ‰é™çš„ç¡¬ä»¶èµ„æºè¿›è¡Œå¾®è°ƒï¼Ÿ(LoRA)

[[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models (arxiv.org)](https://arxiv.org/abs/2106.09685)

## 8. å¦‚ä½•è®©è¯­è¨€æ¨¡å‹æ›´å¥½çš„ç†è§£äººç±»æ„å›¾ï¼Ÿ(instruction tuning)

[arxiv.org/pdf/2203.02155.pdf](https://arxiv.org/pdf/2203.02155.pdf)

# å®è·µéƒ¨åˆ†ï¼ˆè¯¾ç¨‹ä½œä¸šï¼‰ï¼šğŸ¤—

## 0. Chat with LM (optional)

`ChatGLM.py`

å’Œä»»æ„è¯­è¨€æ¨¡å‹èŠå¤© (ChatGPT, ChatGLM, Claude, Bard, Bing......) ã€‚

ä¾‹å¦‚è¯¢é—®ï¼š

```
ä»€ä¹ˆæƒ…å†µä¸‹å•†è´©ä¼šå”®å–æ·¹æ­»çš„é±¼ï¼Ÿ
ä¸¤æ¯50åº¦çš„å·§å…‹åŠ›æ··åˆèµ·æ¥æ˜¯å¤šå°‘åº¦ï¼Ÿ
å¦‚ä½•å‘ç›²äººæ¨é”€VRçœ¼é•œï¼Ÿ
......
```

å¦‚æœæ‚¨å‘ç°äº†æœ‰è¶£çš„ç­”æ¡ˆï¼Œè¯·æäº¤å¹¶åœ¨ç¾¤èŠä¸­å’Œå¤§å®¶åˆ†äº«ã€‚

è¿™é‡Œåˆ—å‡ºäº†ä¸€äº›ğŸ¤—ä¸Šå¯ä»¥æœ¬åœ°éƒ¨ç½²çš„æ¨¡å‹ï¼š

[THUDM/chatglm-6b Â· Hugging Face](https://huggingface.co/THUDM/chatglm-6b)

[THUDM/chatglm2-6b Â· Hugging Face](https://huggingface.co/THUDM/chatglm2-6b)

[meta-llama/Llama-2-7b-chat-hf Â· Hugging Face](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)

æ‚¨ä¹Ÿå¯ä»¥åœ¨ğŸ¤—ä¸Šå¯»æ‰¾å…¶ä»–æ¨¡å‹ï¼Œæˆ–è€…ä½¿ç”¨æ‚¨åœ¨`ä»»åŠ¡8`ä¸­è®­ç»ƒçš„æ¨¡å‹ã€‚

æ‚¨è¿˜å¯ä»¥å°è¯•å’Œä»»æ„èŠå¤©æ¡†å¯¹è¯ï¼Œæ¯”å¦‚ vscode ä¸­çš„ [CodeGeeX](https://marketplace.visualstudio.com/items?itemName=aminer.codegeex) ä»£ç ç”Ÿæˆæ’ä»¶ã€‚

## 1. A + B problem

`A+B_Problem.py`

è¯·å°è¯•å°†ä»¥ä¸‹è¯çš„`Word Embedding`ç›¸åŠ ï¼Œå¹¶æŠ¥å‘Šä¸æ‰€å¾—å‘é‡æœ€æ¥è¿‘çš„è¯ï¼š

```
ç¤ºä¾‹ï¼š
çš‡å-å¥³äºº+ç”·äºº=å›½ç‹ï¼ˆç­”æ¡ˆåœ¨"å­¦ç”Ÿ", "éª‘å£«", "å›½ç‹"ä¸­é€‰æ‹©ï¼‰
ä»¥ä¸‹ä¸ºé¢˜ç›®ï¼š
åƒµå°¸+å¤æ´»=ï¼Ÿï¼ˆç­”æ¡ˆåœ¨"é¬¼é­‚", "äººç±»", "æŠ±æŠ±è„¸"ä¸­é€‰æ‹©ï¼‰
ç§€å‘-å¤´å‘=ï¼Ÿï¼ˆç­”æ¡ˆåœ¨"é—ªäº®", "æ˜æš—", "ç”µç¯"ä¸­é€‰æ‹©ï¼‰
é—ªç”µ-ç”µæµ=ï¼Ÿï¼ˆç­”æ¡ˆåœ¨"å¿«", "é»„è‰²", "ä¸‹é›¨"ä¸­é€‰æ‹©ï¼‰
æ¸…å-åŒ—å¤§=ï¼Ÿï¼ˆç­”æ¡ˆåœ¨"ä¸€æµ", "äºŒæµ", "è´Ÿä¸€æµ"ä¸­é€‰æ‹©ï¼‰
å¤§å­¦+ä¸€æµ+æµ·æ·€=ï¼Ÿï¼ˆç­”æ¡ˆåœ¨"æ¸…å", "åŒ—å¤§", "æ–¯å¦ç¦", "MIT"ä¸­é€‰æ‹©ï¼‰
æ¬§ç›Ÿ-è‹±å›½-æ³•å›½-å¾·å›½=ï¼Ÿï¼ˆç­”æ¡ˆåœ¨"ä¿„ç½—æ–¯", "æ¬§ç›Ÿ", "æ³¢å…°", "ç¾å›½"ä¸­é€‰æ‹©ï¼‰
```

æœ¬æ¬¡ä»»åŠ¡ä¸­ï¼Œå°†ä½¿ç”¨ `bert-base-chinese` çš„`Word Embedding`ã€‚

æ‚¨ä¹Ÿå¯ä»¥æ›´æ¢æ‰€ä½¿ç”¨çš„`Embedding`ï¼Œå¹¶æŠ¥å‘Šå…¶ä»–æœ‰è¶£çš„åŠ å¼ã€‚

## 4. Manual mask filling

`gpt2/modeling_gpt2.py`

ä¸ºäº†è®­ç»ƒåŒå­¦ä»¬çš„`coding`èƒ½åŠ›ï¼Œå‚ç…§`BERT`çš„é¢„è®­ç»ƒæ–¹å¼ï¼Œè®²å¸ˆç”¨è‡ªå·±å°å­¦æœŸä½œä¸šä¸­[éšæœºé‡‡æ ·](https://simple.wikipedia.org/wiki/Infinite_monkey_theorem)çš„ã€[å¯¹æ¯”å­¦ä¹ ](https://arxiv.org/abs/2103.00020)çš„ä»¥åŠ[è‡ªå›å½’ç”Ÿæˆ](https://codegeex.cn/playground)çš„ä»£ç åˆ¶ä½œäº†ä¸€ä¸ªæ•°æ®é›†ï¼Œå¹¶ç”¨`[MASK]`å¯¹éƒ¨åˆ†éœ€è¦é‡æ„çš„ä»£ç ç‰‡æ®µè¿›è¡Œäº†è¦†ç›–ã€‚åŒå­¦ä»¬éœ€è¦åŸºäºè‡ªèº«æˆ–è€…å¤–éƒ¨çŸ¥è¯†åº“ï¼Œå¯¹`[MASK]`è¿›è¡Œè¡¥å…¨ã€‚

ç”±äºè®²å¸ˆä¸çŸ¥é“æ­£ç¡®ç­”æ¡ˆï¼Œå› æ­¤è®²å¸ˆä¼šé€šè¿‡[å¤šæ•°æŠ•ç¥¨](https://arxiv.org/pdf/2305.18290.pdf)çš„æ–¹å¼é€‰æ‹©æœ€ç»ˆ`[MASK]`éƒ¨åˆ†å¡«å†™çš„ä»£ç ã€‚`Bonus`æ­£æ¯”äºè®²å¸ˆå°å­¦æœŸçš„ç»©ç‚¹ã€‚

ç”±äºæœ¬å­¦æœŸè®²å¸ˆæ²¡æœ‰å°å­¦æœŸï¼Œæ‰€ä»¥ä¼šä½¿ç”¨ğŸ¤—ä¸Š`gpt2-base`çš„ä»£ç ä½œä¸ºæœ¬æ¬¡ä½œä¸šçš„æ•°æ®é›†ï¼Œå¯ä»¥ä½¿ç”¨ğŸ¤—ä½œä¸ºå¤–éƒ¨çŸ¥è¯†åº“ã€‚

å¯ä»¥ä½¿ç”¨`Check_gpt2-base.py`è¿›è¡Œæ£€æŸ¥ï¼ˆéœ€ä¸‹è½½æ¨¡å‹è‡³`model/gpt2-base`ï¼‰ã€‚

æäº¤å¡«ç©ºåçš„`gpt2/modeling_gpt2.py`ã€‚

```
# ç¤ºä¾‹ï¼š
class MLP(nn.Module):
Â  Â  def __init__(self, n_state, config):
Â  Â  Â  Â  super().__init__()
Â  Â  Â  Â  nx = config.n_embd  # FFNä¸­é—´ç»´åº¦
Â  Â  Â  Â  self.c_fc = [MASK](n_state, nx)
Â  Â  Â  Â  self.c_proj = [MASK](nx, n_state)
Â  Â  Â  Â  self.act = [MASK]  # æ¿€æ´»å‡½æ•°
Â  Â  Â  Â  self.dropout = nn.Dropout(config.resid_pdrop)
Â  Â  def forward(self, x):
Â  Â  Â  Â  h = self.act([MASK])
Â  Â  Â  Â  h2 = self.c_proj([MASK])
Â  Â  Â  Â  return self.dropout(h2)
```

```
# ç¤ºä¾‹ç­”æ¡ˆ
class MLP(nn.Module):
Â  Â  def __init__(self, n_state, config):
Â  Â  Â  Â  super().__init__()
Â  Â  Â  Â  nx = config.n_embd  # FFNä¸­é—´ç»´åº¦
Â  Â  Â  Â  self.c_fc = Linear(n_state, nx)
Â  Â  Â  Â  self.c_proj = Linear(nx, n_state)
Â  Â  Â  Â  self.act = gelu_new  # gpt2-baseçš„æ¿€æ´»å‡½æ•°
Â  Â  Â  Â  self.dropout = nn.Dropout(config.resid_pdrop)
Â  Â  def forward(self, x):
Â  Â  Â  Â  h = self.act(self.c_fc(x))
Â  Â  Â  Â  h2 = self.c_proj(h)
Â  Â  Â  Â  return self.dropout(h2)
```

æ³¨ï¼šåœ¨ğŸ¤—çš„å®ç°ä¸­ï¼ŒğŸ¤—ä½¿ç”¨äº†Conv1Dä»£æ›¿äº†Linearï¼ˆä¹Ÿè®¸æ˜¯ä¸ºäº†åŠ é€Ÿï¼‰ã€‚

## 5. Pretraining (optional)

é¢„è®­ç»ƒä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼Œç»“æ„ä¸é™(Encoder, Decoder, Encoder-Decoder)ï¼Œæ•°æ®ä¸é™ï¼Œæ¡†æ¶ä¸é™ï¼Œç®—åŠ›ä¸é™ã€‚

ä»¥ä¸‹æ˜¯ä¸€äº›å‚è€ƒï¼š

[GitHub - karpathy/minGPT: A minimal PyTorch re-implementation of the OpenAI GPT (Generative Pretrained Transformer) training](https://github.com/karpathy/minGPT)

[THUDM/SwissArmyTransformer: SwissArmyTransformer is a flexible and powerful library to develop your own Transformer variants. (github.com)](https://github.com/THUDM/SwissArmyTransformer)

[GitHub - microsoft/DeepSpeed: DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.](https://github.com/microsoft/DeepSpeed)

[GitHub - NVIDIA/Megatron-LM: Ongoing research training transformer models at scale](https://github.com/NVIDIA/Megatron-LM)

æœ¬é¡¹ä½œä¸šæ— éœ€æäº¤ã€‚

## 8. Mean(GPT2, GPT4) = GPT3?

`Supervised_Fine-Tuning.py`

æˆ‘ä»¬å°†ä½¿ç”¨ `GPT4` ç”Ÿæˆçš„æ•°æ®å¯¹ `GPT2` è¿›è¡Œ Supervised Fine-Tuning ï¼Œä»¥æ­¤å¸Œæœ› `GPT2` æ¥è¿‘`GPT3` çš„æ°´å¹³ã€‚

å…³äºæ‰€ä½¿ç”¨çš„æ•°æ®ï¼Œæ›´å¤šä¿¡æ¯å‚è§ï¼š[imoneoi/openchat: OpenChat: Less is More for Open-source Models (github.com)](https://github.com/imoneoi/openchat)

ä¸ºäº†å‡å°‘æ˜¾å­˜èµ„æºå ç”¨ï¼Œæˆ‘ä»¬å°†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨LoRAå’Œæ¨¡å‹é‡åŒ–ã€‚

è®­ç»ƒç»“æŸåï¼Œè¿è¡Œ`ChatGPT2-base.py`è¿›è¡Œå¯¹è¯ğŸ¤—ï¼Œæ‚¨è·å¾—äº†GPT[MASK]ï¼Ÿ

æäº¤ä»»æ„å¯¹è¯è®°å½•ï¼Œå¦‚å¾®è°ƒä¸­æœ‰ä»»ä½•é—®é¢˜ï¼Œå¯ä¸€å¹¶æäº¤ã€‚




